import streamlit as st
import numpy as np
import pandas as pd
import joblib
import seaborn as sns
import matplotlib.pyplot as plt
from io import BytesIO
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.decomposition import PCA
import shap

import sklearn
print(sklearn.__version__)

# Ø§ØµÙ„Ø§Ø­ 1: Ù…Ø·Ø§Ø¨Ù‚Øª Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
FEATURES = [
    'Epicardial fat thickness (mm)', 
    'LV mass i (g/m2) Calcolo automatico',
    'LAD (mm)',
    'LVEF (%)',
    'E/e avg', 
    'DM',
    'WHO-FC',
    'PAS (mmHg)', 
    'NT-pro-BNP (pg/mL)', 
    'AF'  # Ø§ØµÙ„Ø§Ø­: AF Ø¨Ù‡ Ø¬Ø§ÛŒ FA
]

NUM_FEATURES = [
    'Epicardial fat thickness (mm)',
    'LV mass i (g/m2) Calcolo automatico', 
    'LAD (mm)',
    'LVEF (%)',
    'E/e avg', 
    'PAS (mmHg)', 
    'NT-pro-BNP (pg/mL)'
]

# Ø§ØµÙ„Ø§Ø­ 2: AF Ø¨Ù‡ Ø¬Ø§ÛŒ FA
CAT_FEATURES = ['DM', 'WHO-FC', 'AF']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ]), NUM_FEATURES),

        ('cat', Pipeline([
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ]), CAT_FEATURES)
    ],
    remainder='drop'
)

full_pipeline = Pipeline([
    ('preprocessing', preprocessor),
    ('pca', PCA(n_components=0.95))
])

st.title("ğŸ«€ HFpEF Probability ğŸ«€")

st.markdown("Do not hesitate to reach us for further questions:")
st.markdown(" ğŸ¢ Dr. Fusco , Manager: info@cardeasrl.it ")
st.markdown(" ğŸ‘©â€ğŸ’» Behshad , programmer : b.ghaseminezhadabdol@studio.unibo.it ")
st.markdown(" ")
st.markdown("  ")
st.markdown("Insert the patient's clinical data below, to detect the probability of having HFpEF. ")

@st.cache_resource
def load_models():
    try:
        pipeline = joblib.load("data_with_pca.joblib")
        log_model = joblib.load("Logistic_model.joblib")
        rf_model = joblib.load("randomforest_model.joblib")
        xgb_model = joblib.load("xgboost_model.joblib")
        return pipeline, log_model, rf_model, xgb_model
    except Exception as e:
        st.error(f"Error loading models: {e}")
        st.stop()

pipeline, log_model, rf_model, xgb_model = load_models()

# Ø§ØµÙ„Ø§Ø­ 3: ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©ØªÚ¯ÙˆØ±ÛŒÚ©Ø§Ù„
user_input = {}

# ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
for feature in NUM_FEATURES:
    if feature == 'LVEF (%)':
        user_input[feature] = st.number_input(f"{feature}:", min_value=0.0, max_value=100.0, step=0.1)
    elif feature == 'PAS (mmHg)':
        user_input[feature] = st.number_input(f"{feature}:", min_value=0.0, max_value=300.0, step=1.0)
    elif feature == 'NT-pro-BNP (pg/mL)':
        user_input[feature] = st.number_input(f"{feature}:", min_value=0.0, step=1.0)
    else:
        user_input[feature] = st.number_input(f"{feature}:", step=0.1)

# ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©ØªÚ¯ÙˆØ±ÛŒÚ©Ø§Ù„
user_input['DM'] = st.selectbox("Diabetes Mellitus (DM):", options=[0, 1], format_func=lambda x: "No" if x == 0 else "Yes")
user_input['WHO-FC'] = st.selectbox("WHO Functional Class:", options=[1, 2, 3, 4])
user_input['AF'] = st.selectbox("Atrial Fibrillation (AF):", options=[0, 1], format_func=lambda x: "No" if x == 0 else "Yes")

if st.button("ğŸ” Estimate ğŸ”"):
    try:
        input_df = pd.DataFrame([user_input])
        
        # Ø§ØµÙ„Ø§Ø­ 4: Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ù‚Ø¨Ù„ Ø§Ø² ØªØ¨Ø¯ÛŒÙ„
        missing_features = set(FEATURES) - set(input_df.columns)
        if missing_features:
            st.error(f"Missing features: {missing_features}")
            st.stop()
        
        # Ù…Ø±ØªØ¨ Ú©Ø±Ø¯Ù† Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        input_df = input_df[FEATURES]
        
        transformed_input = pipeline.transform(input_df)

        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
        prob_log = log_model.predict_proba(transformed_input)[0][1]
        prob_rf = rf_model.predict_proba(transformed_input)[0][1]
        prob_gb = xgb_model.predict_proba(transformed_input)[0][1]

        st.subheader("ğŸ¤” Prediction Probabilities")
        st.write(f"ğŸ”¹ **Logistic Regression**: `{prob_log:.4f}`")
        st.write(f"ğŸ”¹ **Random Forest**: `{prob_rf:.4f}`")
        st.write(f"ğŸ”¹ **XG Boosting**: `{prob_gb:.4f}`")

        # Ø§ØµÙ„Ø§Ø­ 5: ØªØºÛŒÛŒØ± Ø¢Ø³ØªØ§Ù†Ù‡ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ
        if prob_gb > 0.5:  # Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ 0.5 Ø¨Ù‡ØªØ± Ø§Ø² 0.6 Ø§Ø³Øª
            st.error("ğŸš¨ğŸ’€ High Risk of HFpEF Detected! ğŸ˜±")
        else:
            st.success("âœ… Low Risk of HFpEF Detected ğŸ‰")

        # Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        fig, ax = plt.subplots(figsize=(10, 6))
        models = ["Logistic Regression", "Random Forest", "XG Boosting"]
        probabilities = [prob_log, prob_rf, prob_gb]
        bars = ax.bar(models, probabilities, color=['#1f77b4', '#ff7f0e', '#2ca02c'])
        ax.set_title("Model Probability Comparison", fontsize=14, fontweight='bold')
        ax.set_ylabel("HFpEF Probability", fontsize=12)
        ax.set_ylim(0, 1)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø®Ø· Ø¢Ø³ØªØ§Ù†Ù‡
        ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Threshold (0.5)')
        ax.legend()
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±ÙˆÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±
        for bar, prob in zip(bars, probabilities):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{prob:.3f}', ha='center', va='bottom', fontweight='bold')
        
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        st.pyplot(fig)

        # Ø§ØµÙ„Ø§Ø­ 6: SHAP analysis Ø¨Ø±Ø§ÛŒ XGBoost
        try:
            st.subheader("ğŸ¯ SHAP Explanation for this Patient (XGBoost)")
            
            # ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù‚Ø¨Ù„ Ø§Ø² PCA Ø¨Ø±Ø§ÛŒ SHAP
            features_before_pca = pipeline.named_steps['preprocessing'].get_feature_names_out()
            df_for_shap = pd.DataFrame(
                pipeline.named_steps['preprocessing'].transform(input_df), 
                columns=features_before_pca
            )
            
            # Ø§ÛŒØ¬Ø§Ø¯ explainer Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ SHAP values
            explainer = shap.Explainer(xgb_model)
            shap_values = explainer(df_for_shap)
            
            # Ù†Ù…ÙˆØ¯Ø§Ø± SHAP waterfall
            fig, ax = plt.subplots(figsize=(12, 8))
            shap.plots.waterfall(shap_values[0], max_display=10, show=False)
            st.pyplot(fig)
            plt.close()
            
        except Exception as shap_error:
            st.warning(f"SHAP analysis could not be performed: {shap_error}")
            st.info("This might be due to model incompatibility with SHAP or preprocessing issues.")

        # Ø§ØµÙ„Ø§Ø­ 7: Feature importance Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ tree-based
        def plot_feature_importance(model, model_name):
            if hasattr(model, 'feature_importances_'):
                importance = model.feature_importances_
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ù¾Ø³ Ø§Ø² preprocessing
                feature_names = pipeline.named_steps['preprocessing'].get_feature_names_out()
                
                if len(importance) == len(feature_names):
                    importance_df = pd.DataFrame({
                        "Feature": feature_names, 
                        "Importance": importance
                    })
                else:
                    # Ø§Ú¯Ø± PCA Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
                    feature_names = [f"PC_{i+1}" for i in range(len(importance))]
                    importance_df = pd.DataFrame({
                        "Feature": feature_names, 
                        "Importance": importance
                    })
                
                importance_df = importance_df.sort_values(by="Importance", ascending=False)
                
                st.write(f"### Feature Importance - {model_name}")
                fig, ax = plt.subplots(figsize=(10, 8))
                sns.barplot(data=importance_df.head(10), x="Importance", y="Feature", ax=ax)
                ax.set_title(f"Top 10 Feature Importance - {model_name}")
                plt.tight_layout()
                st.pyplot(fig)
                plt.close()
                return fig
            else:
                st.warning(f"{model_name} does not have feature_importances_ attribute")
                return None

        # Ù†Ù…Ø§ÛŒØ´ feature importance Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        plot_feature_importance(rf_model, "Random Forest")
        plot_feature_importance(xgb_model, "XGBoost")

    except Exception as e:
        st.error(f"âŒ Error in prediction: {str(e)}")
        st.error("Please check your input values and try again.")
        
    finally:
        # Ø¨Ø³ØªÙ† ØªÙ…Ø§Ù… figures Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² memory leak
        plt.close('all')
